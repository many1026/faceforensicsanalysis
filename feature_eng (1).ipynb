{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "63e3d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew\n",
    "from sklearn.feature_selection import f_classif # for ANOVA F-test\n",
    "\n",
    "import mediapipe as mp\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import pickle\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d839f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1472a52",
   "metadata": {},
   "source": [
    "### Histogram analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "17f7ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Haar Cascade classifier for face detection.\n",
    "# This file is typically located in the OpenCV data directory.\n",
    "# You might need to adjust this path depending on your OpenCV installation.\n",
    "FACE_CASCADE_PATH = '/workspaces/proyecto/.venv/lib/python3.14/site-packages/cv2/data/' + 'haarcascade_frontalface_default.xml'\n",
    "face_cascade = cv2.CascadeClassifier(FACE_CASCADE_PATH)\n",
    "\n",
    "# --- Check if the cascade file loaded successfully ---\n",
    "if face_cascade.empty():\n",
    "    print(f\"ERROR: Could not load the Haar Cascade classifier from: {FACE_CASCADE_PATH}\")\n",
    "    print(\"Please ensure you have the OpenCV data files installed and the path is correct.\")\n",
    "    # In a real application, you might raise an error here.\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def get_random_face_sequence(video_path, window_size, face_cascade):\n",
    "    \"\"\"\n",
    "    Finds a random continuous sequence of 'window_size' frames, detects the face, \n",
    "    crops the frames to the face bounding box, and converts them to YCbCr color space.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): The path to the video file.\n",
    "        window_size (int): The required number of consecutive frames in the sequence.\n",
    "        face_cascade (cv2.CascadeClassifier): The pre-loaded Haar Cascade classifier.\n",
    "\n",
    "    Returns:\n",
    "        list of np.ndarray: A list of cropped face frames in YCbCr color space, \n",
    "                            or an empty list if no face is detected or the video \n",
    "                            is too short/invalid.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return []\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Determine the starting frame index for the sequence\n",
    "    start_frame = 0\n",
    "    end_frame = total_frames - 1\n",
    "\n",
    "    if total_frames >= window_size:\n",
    "        # Select a random starting frame index for a continuous sequence of 'window_size'\n",
    "        max_start_index = total_frames - window_size\n",
    "        start_frame = random.randint(0, max_start_index)\n",
    "        end_frame = start_frame + window_size - 1\n",
    "    else:\n",
    "        # Video is too short, use the entire video\n",
    "        window_size = total_frames\n",
    "        end_frame = total_frames - 1\n",
    "        \n",
    "    # --- 1. Detect Face in the FIRST frame of the sequence ---\n",
    "    # We assume the face position doesn't change significantly over 'window_size' frames.\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    ret, frame_t_rgb = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        cap.release()\n",
    "        return []\n",
    "\n",
    "    # Convert frame to grayscale for faster face detection\n",
    "    gray_t = cv2.cvtColor(frame_t_rgb, cv2.COLOR_BGR2GRAY) \n",
    "    \n",
    "    # Detect faces. Returns (x, y, w, h) for each face.\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray_t, \n",
    "        scaleFactor=1.1, \n",
    "        minNeighbors=5, \n",
    "        minSize=(30, 30)\n",
    "    )\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        # Skip if no face is detected in the starting frame\n",
    "        cap.release()\n",
    "        return []\n",
    "    \n",
    "    # Use the largest detected face (x, y, w, h)\n",
    "    x, y, w, h = max(faces, key=lambda f: f[2] * f[3])\n",
    "\n",
    "    # --- 2. Extract and Crop all Frames in the Sequence ---\n",
    "    face_sequence_ycbcr = []\n",
    "    \n",
    "    # Reset to the start of the sequence\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "    for _ in range(window_size):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            # Should not happen if total_frames was calculated correctly, but safe check\n",
    "            break \n",
    "            \n",
    "        # Crop to the face bounding box\n",
    "        cropped_frame_rgb = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Convert to YCbCr color space\n",
    "        cropped_frame_ycbcr = cv2.cvtColor(cropped_frame_rgb, cv2.COLOR_BGR2YCrCb)\n",
    "        \n",
    "        face_sequence_ycbcr.append(cropped_frame_ycbcr)\n",
    "\n",
    "    cap.release()\n",
    "    return face_sequence_ycbcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "21087276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emd_temporal_features_ycc(face_sequence_ycbcr, window_size=30, n_bins=64):\n",
    "    \"\"\"\n",
    "    Calcula la Distancia EMD de cada histograma a la media temporal de la secuencia \n",
    "    en los canales de color Y, Cb y Cr, y extrae estadísticas de la variación.\n",
    "\n",
    "    Args:\n",
    "        face_sequence_ycbcr (list): Lista de arreglos numpy (cuadros) en espacio YCbCr,\n",
    "                                    ya recortados al bounding box de la cara.\n",
    "        window_size (int): Longitud de la secuencia (N).\n",
    "        n_bins (int): Número de bins para el histograma.\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con las estadísticas (Media, Varianza, Rango) \n",
    "              de las variaciones EMD para Y, Cb y Cr (9 características en total).\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(face_sequence_ycbcr) != window_size:\n",
    "        window_size = len(face_sequence_ycbcr)\n",
    "        if window_size == 0:\n",
    "            return {f'emd_{c}_mean': 0.0 for c in ['y', 'cb', 'cr']} # Retorno seguro\n",
    "\n",
    "    channels = [0, 1, 2] # 0=Y, 1=Cb, 2=Cr\n",
    "    channel_names = ['y', 'cb', 'cr']\n",
    "    \n",
    "    all_hists = {name: [] for name in channel_names}\n",
    "    \n",
    "    hist_range = [0, 256] \n",
    "    \n",
    "    # --- 1. Cálculo de Histograma para cada canal de cada cuadro ---\n",
    "    for frame in face_sequence_ycbcr:\n",
    "        for i, name in zip(channels, channel_names):\n",
    "            # Extraer histograma para el canal i\n",
    "            hist = cv2.calcHist([frame], [i], None, [n_bins], hist_range)\n",
    "            # Normalizar y aplanar\n",
    "            hist = cv2.normalize(hist, hist).flatten() \n",
    "            all_hists[name].append(hist)\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    # --- 2. Iterar sobre cada canal (Y, Cb, Cr) para el análisis EMD ---\n",
    "    for name in channel_names:\n",
    "        hists_array = np.array(all_hists[name])\n",
    "        \n",
    "        # Cálculo del Histograma Promedio Temporal (H_bar)\n",
    "        h_bar = np.mean(hists_array, axis=0)\n",
    "\n",
    "        variation_emd = []\n",
    "        \n",
    "        # Convertir a float32 para la función cv2.compareHist\n",
    "        h_bar_f32 = h_bar.astype('float32')\n",
    "\n",
    "        EMD_COMPARE_METHOD = 3  # cv2.HISTCMP_EMD\n",
    "\n",
    "        # Cálculo de las Variaciones EMD respecto a la media\n",
    "        for i in range(window_size):\n",
    "            hist_f32 = hists_array[i].astype('float32')\n",
    "            \n",
    "            # Usamos cv2.HISTCMP_EMD (Distancia de Earth Mover)\n",
    "            emd_value = cv2.compareHist(hist_f32, h_bar_f32, EMD_COMPARE_METHOD)\n",
    "            variation_emd.append(emd_value)\n",
    "\n",
    "        # --- 3. Extracción de Métricas Estadísticas de la Variación EMD ---\n",
    "        var_array = np.array(variation_emd)\n",
    "        \n",
    "        results[f'emd_{name}_mean'] = np.mean(var_array)\n",
    "        results[f'emd_{name}_variance'] = np.var(var_array)\n",
    "        results[f'emd_{name}_range'] = np.max(var_array) - np.min(var_array)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d18efb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histogram_analysis(root_dir, window_size, videos_per_category, num_bins=64):\n",
    "\n",
    "    all_histograms = {}\n",
    "    category_dirs = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "\n",
    "    # Loop through each category\n",
    "    for category in category_dirs:\n",
    "        print(f\"--- Processing category: **{category}** ---\")\n",
    "        category_path = os.path.join(root_dir, category)\n",
    "        all_video_paths = glob.glob(os.path.join(category_path, '*.mp4')) \n",
    "        \n",
    "        # Sample videos if there are more than required\n",
    "        if len(all_video_paths) > videos_per_category:\n",
    "            video_paths = random.sample(all_video_paths, videos_per_category)\n",
    "        else:\n",
    "            video_paths = all_video_paths\n",
    "        \n",
    "        category_features = {}\n",
    "        \n",
    "        # Process each video\n",
    "        for video_path in video_paths:\n",
    "            \n",
    "            # --- MODULAR CALL: Get the face sequence in YCbCr ---\n",
    "            face_sequence_ycbcr = get_random_face_sequence(video_path, window_size, face_cascade)\n",
    "            \n",
    "            if not face_sequence_ycbcr:\n",
    "                # No face detected or video invalid/too short\n",
    "                continue\n",
    "            \n",
    "            # --- MODULAR CALL: Extract EMD Temporal Features ---\n",
    "            emd_features = extract_emd_temporal_features_ycc(face_sequence_ycbcr, window_size, num_bins)\n",
    "            category_features[video_path] = emd_features\n",
    "        \n",
    "        all_histograms[category] = category_features\n",
    "        print(f\"Processed **{len(video_paths)}** videos. Total **{len(category_features)}** face-focused features extracted.\")\n",
    "\n",
    "    return all_histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441aca22",
   "metadata": {},
   "source": [
    "#### Llamada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "69c0dcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing category: **Deepfakes** ---\n",
      "Processed **250** videos. Total **248** face-focused features extracted.\n",
      "--- Processing category: **Original** ---\n",
      "Processed **250** videos. Total **250** face-focused features extracted.\n",
      "--- Processing category: **FaceShifter** ---\n",
      "Processed **250** videos. Total **250** face-focused features extracted.\n",
      "--- Processing category: **FaceSwap** ---\n",
      "Processed **250** videos. Total **248** face-focused features extracted.\n",
      "--- Processing category: **NeuralTextures** ---\n",
      "Processed **250** videos. Total **250** face-focused features extracted.\n",
      "--- Processing category: **Face2Face** ---\n",
      "Processed **250** videos. Total **249** face-focused features extracted.\n"
     ]
    }
   ],
   "source": [
    "random.seed(SEED)\n",
    "VIDEO_ROOT_DIR = '/workspaces/proyecto/workspace/src_c40'\n",
    "WINDOW_SIZE = 100\n",
    "VIDEOS_PER_CATEGORY = 250\n",
    "NUM_BINS = 64\n",
    "\n",
    "histogram_features = get_histogram_analysis(VIDEO_ROOT_DIR, WINDOW_SIZE, VIDEOS_PER_CATEGORY, NUM_BINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "688d4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_from_dictionary(features, imputer_strategy = None, imputer_fill_value = None):\n",
    "    rows = []\n",
    "    for category, features_dict in features.items():\n",
    "        for video, features in features_dict.items():\n",
    "            # Keep the dictionary structure to preserve column names\n",
    "            row = features.copy()\n",
    "            row['label'] = category\n",
    "            row['video_id'] = video\n",
    "            rows.append(row)\n",
    "    \n",
    "    if imputer_strategy == None:\n",
    "        df =  pd.DataFrame(rows)\n",
    "        return df.set_index('video_id')\n",
    "    \n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy=imputer_strategy, fill_value=imputer_fill_value)\n",
    "    return imputer.fit_transform(pd.DataFrame(rows)) \n",
    "\n",
    "def pca_and_plot(features: dict, imputer_strategy = None, imputer_fill_value = None):\n",
    "\n",
    "    try:\n",
    "        # Convertir las características del histograma a un DataFrame\n",
    "        df = get_dataframe_from_dictionary(features, imputer_strategy, imputer_fill_value)\n",
    "        # Separar características y etiquetas\n",
    "        X = df.drop(columns=['label']).values\n",
    "        y = df['label'].values\n",
    "        # Aplicar PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(X)\n",
    "    except ValueError:\n",
    "        df = get_dataframe_from_dictionary(features).dropna()\n",
    "        X = df.drop(columns=['label']).values\n",
    "        y = df['label'].values\n",
    "        X_pca = pca.fit_transform(X)\n",
    "\n",
    "    # Crear un DataFrame para los resultados PCA\n",
    "    pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "    pca_df['label'] = y\n",
    "\n",
    "    # Graficar los resultados PCA\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for category in pca_df['label'].unique():\n",
    "        subset = pca_df[pca_df['label'] == category]\n",
    "        plt.scatter(subset['PC1'], subset['PC2'], label=category)\n",
    "    \n",
    "    plt.title('PCA of Histogram Features')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot explained variance for all PCA components\n",
    "    pca_full = PCA()  # fit PCA with all possible components\n",
    "    pca_full.fit(X)\n",
    "    explained = pca_full.explained_variance_ratio_\n",
    "    cumulative = np.cumsum(explained)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(np.arange(1, len(explained) + 1), explained, alpha=0.7, label='Individual explained variance')\n",
    "    plt.plot(np.arange(1, len(explained) + 1), cumulative, color='r', marker='o', label='Cumulative explained variance')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('Explained Variance by PCA Components')\n",
    "    plt.xticks(np.arange(1, len(explained) + 1))\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bfb345",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_and_plot(histogram_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f7b2722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple random forrest classifier to validate feature usefulness\n",
    "def validate_features_with_random_forest(features_dataframe):\n",
    "    \"\"\"\n",
    "    Validates the usefulness of extracted features using a Random Forest classifier.\n",
    "\n",
    "    Args:\n",
    "        features_dataframe: a dataframe where each row corresponds to a sample,\n",
    "                            columns are features, and there is a 'label' column for categories.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Validating Features with Random Forest Classifier ---\")\n",
    "\n",
    "    # 1. Prepare Data\n",
    "    all_features = features_dataframe.drop(columns=['label']).values\n",
    "    all_labels = features_dataframe['label'].values\n",
    "    \n",
    "    X = np.array(all_features)\n",
    "    y = np.array(all_labels)\n",
    "\n",
    "    if X.ndim != 2 or X.shape[1] == 0:\n",
    "        print(\"Error: Feature matrix X is empty or incorrectly shaped.\")\n",
    "        return\n",
    "\n",
    "    # 2. Split Data into Training and Testing Sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # 3. Initialize and Train Random Forest Classifier\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # 4. Make Predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # 5. Evaluate the Classifier\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b86df",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_features_with_random_forest(get_dataframe_from_dictionary(histogram_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d3a4f",
   "metadata": {},
   "source": [
    "### Facial landmarks analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d6c9c1",
   "metadata": {},
   "source": [
    "#### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8ebffe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 0. Extraer landmarks\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "def get_landmark_sequence(video_path: str, window_size: int) -> list:\n",
    "    \"\"\"\n",
    "    Extrae una secuencia de landmarks faciales de un video usando MediaPipe Face Mesh.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Ruta al archivo de video.\n",
    "        window_size (int): Número de cuadros a extraer.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de objetos 'NormalizedLandmarkList' (uno por cuadro).\n",
    "    \"\"\"\n",
    "    landmark_sequence = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: No se pudo abrir el video {video_path}\")\n",
    "        return landmark_sequence\n",
    "\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = random.sample(range(total_frames), min(window_size, total_frames))\n",
    "\n",
    "    for frame_idx in sorted(frame_indices):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(rgb_frame)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            landmark_sequence.append(results.multi_face_landmarks[0])\n",
    "    \n",
    "    cap.release()\n",
    "    return landmark_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6ce14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1765298659.747197   70927 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1765298659.757550   70927 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 1. CONSTANTES Y CONFIGURACIÓN\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "# Configuration for the face mesh model\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False, \n",
    "    max_num_faces=1, \n",
    "    refine_landmarks=True,  # Use a refined model for better accuracy\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Índices EAR (p1, p2, p3, p4, p5, p6) para el cálculo 3D\n",
    "LEFT_EYE_INDICES_EAR = [33, 160, 158, 133, 153, 144] \n",
    "RIGHT_EYE_INDICES_EAR = [362, 385, 387, 263, 373, 380] \n",
    "\n",
    "EAR_BLINK_THRESHOLD = 0.25      # Umbral del Eye Aspect Ratio (EAR) para ojo cerrado.\n",
    "MIN_CONSECUTIVE_FRAMES = 2      # Mínimo de cuadros cerrados para registrar un parpadeo.\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2. FUNCIONES DE CÁLCULO BÁSICO (3D OPTIMIZADO)\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "def euclidean_distance(p1: np.ndarray, p2: np.ndarray) -> float:\n",
    "    return np.linalg.norm(p1 - p2)\n",
    "\n",
    "def calculate_ear(eye_landmarks: list) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el Eye Aspect Ratio (EAR) para un ojo dado sus 6 puntos 3D.\n",
    "    \n",
    "    EAR = (|p2-p6| + |p3-p5|) / (2 * |p1-p4|)\n",
    "    \n",
    "    Args:\n",
    "        eye_landmarks: Lista de 6 puntos [p1, ..., p6] como arrays (x, y, z).\n",
    "    \n",
    "    Returns:\n",
    "        El valor del EAR 3D.\n",
    "    \"\"\"\n",
    "    # p1, p2, p3, p4, p5, p6 corresponden a índices 0 a 5\n",
    "    \n",
    "    # Distancias verticales (p2-p6 y p3-p5)\n",
    "    vertical_sum = euclidean_distance(eye_landmarks[1], eye_landmarks[5]) + \\\n",
    "                   euclidean_distance(eye_landmarks[2], eye_landmarks[4])\n",
    "    \n",
    "    # Distancia horizontal (p1-p4)\n",
    "    horizontal_distance = euclidean_distance(eye_landmarks[0], eye_landmarks[3])\n",
    "    \n",
    "    if horizontal_distance == 0:\n",
    "        return 0.0\n",
    "        \n",
    "    ear = vertical_sum / (2.0 * horizontal_distance)\n",
    "    return ear\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3. FUNCIÓN PRINCIPAL DE ANÁLISIS DE LANDMARKS POR FRAME (CONCISA)\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "def analyze_frame_landmarks(face_landmarks) -> float:\n",
    "    \"\"\"\n",
    "    Extrae los 6 puntos clave de cada ojo y calcula el EAR mínimo en 3D.\n",
    "    \n",
    "    Args:\n",
    "        face_landmarks: Un objeto 'NormalizedLandmarkList' de MediaPipe Face Mesh. \n",
    "\n",
    "    Returns:\n",
    "        El EAR mínimo entre el ojo izquierdo y derecho. Retorna 1.0 si no hay landmarks.\n",
    "    \"\"\"\n",
    "    if not face_landmarks:\n",
    "        raise ValueError(\"No face landmarks detected in this frame.\")\n",
    "\n",
    "    def get_eye_coords(indices):\n",
    "        \"\"\"Usa comprensión de listas para extraer puntos (x, y, z) de forma concisa.\"\"\"\n",
    "        return [np.array([face_landmarks.landmark[i].x, \n",
    "                          face_landmarks.landmark[i].y, \n",
    "                          face_landmarks.landmark[i].z]) \n",
    "                for i in indices]\n",
    "\n",
    "    # Extracción concisa y cálculo 3D\n",
    "    left_eye_coords = get_eye_coords(LEFT_EYE_INDICES_EAR)\n",
    "    right_eye_coords = get_eye_coords(RIGHT_EYE_INDICES_EAR)\n",
    "        \n",
    "    ear_left = calculate_ear(left_eye_coords)\n",
    "    ear_right = calculate_ear(right_eye_coords)\n",
    "\n",
    "    return min(ear_left, ear_right)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4. FUNCIÓN DE EXTRACCIÓN DE CARACTERÍSTICAS DE PARPADEO\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "def extract_blink_features(landmark_sequence: list) -> dict:\n",
    "    \"\"\"\n",
    "    Calcula las 6 métricas de parpadeo a partir de una secuencia de resultados \n",
    "    de análisis de EAR.\n",
    "    \n",
    "    Args:\n",
    "        landmark_sequence (list): Lista de objetos 'NormalizedLandmarkList' (un elemento por frame).\n",
    "        fps (float): Frames por segundo de la secuencia de video.\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con las 6 características de parpadeo solicitadas.\n",
    "    \"\"\"\n",
    "    \n",
    "    frame_count = len(landmark_sequence)\n",
    "\n",
    "    if not landmark_sequence or frame_count == 0:\n",
    "        return {\n",
    "            'blink_frequency': 0.0,\n",
    "            'interval_variance': 0.0, 'interval_skewness': 0.0,\n",
    "            'duration_mean': 0.0, 'duration_variance': 0.0, 'duration_range': 0.0\n",
    "        }\n",
    "\n",
    "    # 1. Generación de la Serie Temporal EAR\n",
    "    ears_min = []\n",
    "    \n",
    "    for lm in landmark_sequence:\n",
    "        try:\n",
    "            ears_min.append(analyze_frame_landmarks(lm))\n",
    "        except ValueError:\n",
    "            # Se continúa con el siguiente frame.\n",
    "            pass \n",
    "\n",
    "    # Se continúa con la lógica solo si quedaron cuadros válidos\n",
    "    if not ears_min:\n",
    "        print(\"Advertencia: Ningún frame pudo ser analizado tras la detección.\")\n",
    "        return extract_blink_features([])\n",
    "\n",
    "    # 2. Detección de Parpadeos y Extracción de Duraciones/Intervalos\n",
    "    blink_durations_frames = []      \n",
    "    intervals_frames = []            \n",
    "    \n",
    "    is_blinking = False\n",
    "    current_blink_duration = 0      \n",
    "    frames_since_last_blink = 0     \n",
    "    total_blinks = 0\n",
    "    \n",
    "    for ear in ears_min:\n",
    "        frames_since_last_blink += 1 \n",
    "        is_closed = ear < EAR_BLINK_THRESHOLD\n",
    "        \n",
    "        if is_closed:\n",
    "            current_blink_duration += 1\n",
    "            if not is_blinking:\n",
    "                is_blinking = True\n",
    "        else:\n",
    "            if is_blinking:\n",
    "                if current_blink_duration >= MIN_CONSECUTIVE_FRAMES:\n",
    "                    total_blinks += 1\n",
    "                    blink_durations_frames.append(current_blink_duration)\n",
    "                    # El intervalo es el tiempo abierto antes de este parpadeo\n",
    "                    intervals_frames.append(frames_since_last_blink - current_blink_duration)\n",
    "                \n",
    "                # Resetear el estado\n",
    "                is_blinking = False\n",
    "                current_blink_duration = 0\n",
    "                frames_since_last_blink = 0 \n",
    "\n",
    "    # 3. Cálculo de Métricas Finales\n",
    "    frame_count = len(ears_min)\n",
    "    results = {}\n",
    "    \n",
    "    # A. Frecuencia de Parpadeo\n",
    "    results['total_blinks'] = total_blinks\n",
    "    results['blink_frequency'] = total_blinks / frame_count if frame_count > 0 else 0.0\n",
    "\n",
    "    # B. Intervalo entre Parpadeos\n",
    "    if intervals_frames and len(intervals_frames) >= 2:\n",
    "        results['interval_variance'] = np.var(intervals_frames)\n",
    "        results['interval_skewness'] = skew(intervals_frames)\n",
    "    else:\n",
    "        results['interval_variance'] = 0.0\n",
    "        results['interval_skewness'] = 0.0\n",
    "\n",
    "    # C. Duración de Parpadeo\n",
    "    if blink_durations_frames:\n",
    "        results['duration_mean'] = np.mean(blink_durations_frames)\n",
    "        results['duration_variance'] = np.var(blink_durations_frames)\n",
    "        results['duration_range'] = np.max(blink_durations_frames) - np.min(blink_durations_frames)\n",
    "    else:\n",
    "        results['duration_mean'] = 0.0\n",
    "        results['duration_variance'] = 0.0\n",
    "        results['duration_range'] = 0.0\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ae878e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Índices Clave de MediaPipe Face Mesh ---\n",
    "# NARIZ: Puntos centrales de la punta de la nariz\n",
    "NOSE_POINT = 4\n",
    "# OREJAS/Caras Laterales: Puntos de las caras laterales (orejas conceptuales)\n",
    "LEFT_FACE_POINT = 133\n",
    "RIGHT_FACE_POINT = 362\n",
    "# FRENTE: Punto central superior (frente conceptual)\n",
    "FOREHEAD_POINT = 10\n",
    "# BOCA: Punto central superior del labio (filtrum superior)\n",
    "UPPER_MOUTH_POINT = 0\n",
    "\n",
    "def extract_facial_distance_features(landmark_sequence: list) -> dict:\n",
    "    \"\"\"\n",
    "    Calcula y normaliza las distancias clave de la nariz (orejas, frente, boca) \n",
    "    para una secuencia de landmarks, y extrae estadísticas de la serie.\n",
    "\n",
    "    Args:\n",
    "        landmark_sequence (list): Lista de objetos 'NormalizedLandmarkList' (un elemento por frame).\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con el máximo, mínimo y asimetría (skewness) de cada serie de distancia.\n",
    "    \"\"\"\n",
    "    if not landmark_sequence:\n",
    "        return {}\n",
    "\n",
    "    series = {\n",
    "        'nose_to_left_ear': [],\n",
    "        'nose_to_right_ear': [],\n",
    "        'nose_to_forehead': [],\n",
    "        'nose_to_upper_mouth': []\n",
    "    }\n",
    "\n",
    "    # --- 1. Procesamiento Frame a Frame ---\n",
    "    for face_landmarks in landmark_sequence:\n",
    "        \n",
    "        # Validación de landmarks (manejo de errores robusto)\n",
    "        if not face_landmarks or not face_landmarks.landmark:\n",
    "            # Omitir frames sin detección (el llamador ya maneja la excepción del frame completo)\n",
    "            continue\n",
    "        \n",
    "        # Función auxiliar para obtener el punto 3D\n",
    "        def get_point(index):\n",
    "            lm = face_landmarks.landmark[index]\n",
    "            return np.array([lm.x, lm.y, lm.z])\n",
    "        \n",
    "        try:\n",
    "            # a. Extracción de Puntos\n",
    "            p_nose = get_point(NOSE_POINT)\n",
    "            p_left_face = get_point(LEFT_FACE_POINT)\n",
    "            p_right_face = get_point(RIGHT_FACE_POINT)\n",
    "            p_forehead = get_point(FOREHEAD_POINT)\n",
    "            p_upper_mouth = get_point(UPPER_MOUTH_POINT)\n",
    "\n",
    "            # b. Distancia de Normalización (Ancho Horizontal del Rostro)\n",
    "            # Normalizar con la distancia entre las caras laterales.\n",
    "            normalization_distance = euclidean_distance(p_left_face, p_right_face)\n",
    "            \n",
    "            # Evitar división por cero\n",
    "            if normalization_distance == 0:\n",
    "                continue\n",
    "\n",
    "            # c. Cálculo de Distancias y Normalización\n",
    "            dist_left_ear = euclidean_distance(p_nose, p_left_face) / normalization_distance\n",
    "            dist_right_ear = euclidean_distance(p_nose, p_right_face) / normalization_distance\n",
    "            dist_forehead = euclidean_distance(p_nose, p_forehead) / normalization_distance\n",
    "            dist_upper_mouth = euclidean_distance(p_nose, p_upper_mouth) / normalization_distance\n",
    "\n",
    "            # d. Almacenamiento\n",
    "            series['nose_to_left_ear'].append(dist_left_ear)\n",
    "            series['nose_to_right_ear'].append(dist_right_ear)\n",
    "            series['nose_to_forehead'].append(dist_forehead)\n",
    "            series['nose_to_upper_mouth'].append(dist_upper_mouth)\n",
    "\n",
    "        except IndexError:\n",
    "            # Esto puede ocurrir si un punto clave no existe por alguna razón (poco común)\n",
    "            continue\n",
    "\n",
    "    # --- 2. Cálculo de Métricas Finales ---\n",
    "    final_metrics = {}\n",
    "    \n",
    "    for name, data_series in series.items():\n",
    "        # Calcular las métricas solo si hay datos válidos en la serie\n",
    "        if data_series:\n",
    "            arr = np.array(data_series)\n",
    "            \n",
    "            # Máximo\n",
    "            final_metrics[f'{name}_max'] = np.max(arr)\n",
    "            # Mínimo\n",
    "            final_metrics[f'{name}_min'] = np.min(arr)\n",
    "            # Asimetría (Skewness)\n",
    "            # La asimetría requiere al menos 3 puntos para ser significativa, pero funciona con 2+\n",
    "            final_metrics[f'{name}_skewness'] = skew(arr)\n",
    "        else:\n",
    "            # Si la serie está vacía, se retorna 0.0 o un valor neutro\n",
    "            final_metrics[f'{name}_max'] = 0.0\n",
    "            final_metrics[f'{name}_min'] = 0.0\n",
    "            final_metrics[f'{name}_skewness'] = 0.0\n",
    "\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772bbd21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6559748c",
   "metadata": {},
   "source": [
    "#### Llamadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c66f6566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to open pickled file; extracting landmarks.\n",
      "   ***** Extracting landmarks from videos *****\n",
      "--- Processing category: **Deepfakes** ---\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/Deepfakes/817_827.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/Deepfakes/923_023.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/Deepfakes/427_637.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/Deepfakes/759_755.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/Deepfakes/862_047.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/Deepfakes/254_261.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/Deepfakes/776_676.mp4\n",
      "Processed **250** videos. Total **243** landmark sequences extracted.\n",
      "--- Processing category: **Original** ---\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/Original/824.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/Original/647.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/Original/923.mp4\n",
      "Processed **250** videos. Total **247** landmark sequences extracted.\n",
      "--- Processing category: **FaceShifter** ---\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/FaceShifter/722_458.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/FaceShifter/221_206.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/FaceShifter/647_622.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/FaceShifter/880_135.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/FaceShifter/033_097.mp4\n",
      "Processed **250** videos. Total **245** landmark sequences extracted.\n",
      "--- Processing category: **FaceSwap** ---\n",
      "Processed **250** videos. Total **250** landmark sequences extracted.\n",
      "--- Processing category: **NeuralTextures** ---\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/NeuralTextures/880_135.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/NeuralTextures/551_631.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/NeuralTextures/647_622.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/NeuralTextures/722_458.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/NeuralTextures/923_023.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/NeuralTextures/281_474.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/NeuralTextures/221_206.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/NeuralTextures/759_755.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/NeuralTextures/776_676.mp4\n",
      "Processed **250** videos. Total **241** landmark sequences extracted.\n",
      "--- Processing category: **Face2Face** ---\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/Face2Face/551_631.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/Face2Face/237_236.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/Face2Face/427_637.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/Face2Face/776_676.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/Face2Face/456_435.mp4\n",
      "No landmarks for video /workspaces/proyecto/workspace/src_c40/Face2Face/254_261.mp4\n",
      "Processed **250** videos. Total **244** landmark sequences extracted.\n",
      "Landmarks successfully pickled to allLandmarks_250_with_IDs.pkl\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "#  Extracción de landmarks\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "random.seed(SEED)\n",
    "PATH = '/workspaces/proyecto/workspace/src_c40'\n",
    "SAMPLE_VIDEOS_PER_CATEGORY = 250\n",
    "WINDOW_SIZE = 100\n",
    "PICKLED_FILE_PATH = \"\"\n",
    "PICKLED_FILE_NAME = f\"allLandmarks_{SAMPLE_VIDEOS_PER_CATEGORY}_with_IDs.pkl\"\n",
    "\n",
    "try:\n",
    "    with open(PICKLED_FILE_NAME, 'rb') as file:\n",
    "        all_landmarks = pickle.load(file)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Unable to open pickled file; extracting landmarks.\")\n",
    "    all_landmarks = {}\n",
    "    category_dirs = [d for d in os.listdir(PATH) if os.path.isdir(os.path.join(PATH, d))]\n",
    "    print(\"   ***** Extracting landmarks from videos *****\")\n",
    "    for category in category_dirs:\n",
    "        print(f\"--- Processing category: **{category}** ---\")\n",
    "        category_path = os.path.join(PATH, category)\n",
    "        video_paths = glob.glob(os.path.join(category_path, '*.mp4')) \n",
    "        if len(video_paths) > SAMPLE_VIDEOS_PER_CATEGORY:\n",
    "            video_paths = random.sample(video_paths, SAMPLE_VIDEOS_PER_CATEGORY)\n",
    "        \n",
    "        category_landmarks = {}\n",
    "        \n",
    "        for video_path in video_paths:\n",
    "            landmark_seq = get_landmark_sequence(video_path, window_size=WINDOW_SIZE)\n",
    "            if not landmark_seq:\n",
    "                print(f\"No landmarks for video {video_path}\")\n",
    "                continue\n",
    "            category_landmarks[video_path] = landmark_seq\n",
    "\n",
    "        all_landmarks[category] = category_landmarks\n",
    "        \n",
    "        print(f\"Processed **{len(video_paths)}** videos. Total **{len(category_landmarks)}** landmark sequences extracted.\")\n",
    "\n",
    "    with open(PICKLED_FILE_NAME, 'wb') as file:\n",
    "        pickle.dump(all_landmarks, file)\n",
    "\n",
    "    print(f\"Landmarks successfully pickled to {PICKLED_FILE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f772611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ***** Extracting features from landmarks *****\n",
      "--- Processing category: **Deepfakes** ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20254/1161925256.py:172: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  results['interval_skewness'] = skew(intervals_frames)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed **243** landmark sequences. Total **243** blink features extracted.\n",
      "--- Processing category: **Original** ---\n",
      "Processed **247** landmark sequences. Total **247** blink features extracted.\n",
      "--- Processing category: **FaceShifter** ---\n",
      "Processed **245** landmark sequences. Total **245** blink features extracted.\n",
      "--- Processing category: **FaceSwap** ---\n",
      "Processed **250** landmark sequences. Total **250** blink features extracted.\n",
      "--- Processing category: **NeuralTextures** ---\n",
      "Processed **241** landmark sequences. Total **241** blink features extracted.\n",
      "--- Processing category: **Face2Face** ---\n",
      "Processed **244** landmark sequences. Total **244** blink features extracted.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "#  Análisis de parpadeos\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "all_blink_features = {}\n",
    "print(\"   ***** Extracting features from landmarks *****\")\n",
    "for category in all_landmarks:\n",
    "    print(f\"--- Processing category: **{category}** ---\")\n",
    "    category_features = {}\n",
    "    \n",
    "    for video_path, landmark_seq in all_landmarks[category].items():\n",
    "        blink_features = extract_blink_features(landmark_seq)\n",
    "        category_features[video_path] = blink_features\n",
    "        \n",
    "    all_blink_features[category] = category_features\n",
    "\n",
    "    print(f\"Processed **{len(all_landmarks[category])}** landmark sequences. Total **{len(category_features)}** blink features extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e64b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_and_plot(all_blink_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8b414355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ***** Extracting features from landmarks *****\n",
      "--- Processing category: **Deepfakes** ---\n",
      "Processed **243** landmark sequences. Total **243** facial distance features extracted.\n",
      "--- Processing category: **Original** ---\n",
      "Processed **247** landmark sequences. Total **247** facial distance features extracted.\n",
      "--- Processing category: **FaceShifter** ---\n",
      "Processed **245** landmark sequences. Total **245** facial distance features extracted.\n",
      "--- Processing category: **FaceSwap** ---\n",
      "Processed **250** landmark sequences. Total **250** facial distance features extracted.\n",
      "--- Processing category: **NeuralTextures** ---\n",
      "Processed **241** landmark sequences. Total **241** facial distance features extracted.\n",
      "--- Processing category: **Face2Face** ---\n",
      "Processed **244** landmark sequences. Total **244** facial distance features extracted.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "#  Análisis de distancias\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "all_distance_features = {}\n",
    "print(\"   ***** Extracting features from landmarks *****\")\n",
    "for category in all_landmarks:\n",
    "    print(f\"--- Processing category: **{category}** ---\")\n",
    "    category_features = {}\n",
    "    \n",
    "    for video_path, landmark_seq in all_landmarks[category].items():\n",
    "        distance_features = extract_facial_distance_features(landmark_seq)\n",
    "        category_features[video_path] = distance_features\n",
    "\n",
    "    all_distance_features[category] = category_features\n",
    "\n",
    "    print(f\"Processed **{len(all_landmarks[category])}** landmark sequences. Total **{len(category_features)}** facial distance features extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522230a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_and_plot(all_distance_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dbfdca",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not a mapping",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m all_features[category] = []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sequence1, sequence2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_blink_features[category], all_distance_features[category]):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     all_features[category].append({**sequence1, **sequence2})\n",
      "\u001b[31mTypeError\u001b[39m: 'str' object is not a mapping"
     ]
    }
   ],
   "source": [
    "all_features = {}\n",
    "for category in all_blink_features:\n",
    "    all_features[category] = {}\n",
    "    for sequence1, sequence2 in zip(all_blink_features[category], all_distance_features[category]):\n",
    "        all_features[category].append({**sequence1, **sequence2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f1fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_features_with_random_forest(get_dataframe_from_dictionary(all_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effe82d7",
   "metadata": {},
   "source": [
    "### Combinar variables de histogramas y de razgos faciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfd7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h = get_dataframe_from_dictionary(histogram_features)\n",
    "df_l = get_dataframe_from_dictionary(all_features)\n",
    "# --------------------------------------------------------------------\n",
    "#  Export dataframes to CSV\n",
    "# -------------------------------------------------------------------- \n",
    "\n",
    "df_h.to_csv('histogram_features_with_ids.csv', index=False)\n",
    "df_l.to_csv('landmark_features_with_ids.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bcb32795",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_df = pd.merge(get_dataframe_from_dictionary(all_distance_features), get_dataframe_from_dictionary(all_blink_features), left_index=True, right_index=True)\n",
    "all_features_df.drop(columns=['label_x'], inplace=True)\n",
    "all_features_df.rename(columns={'label_y': 'label'}, inplace=True)\n",
    "all_features_df.to_csv('landmark_features_with_ids.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "49a85446",
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_features_df = get_dataframe_from_dictionary(histogram_features)\n",
    "histogram_features_df.to_csv(\"histogram_features_with_ids.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787dea97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ac857d1",
   "metadata": {},
   "source": [
    "#### Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc336012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def extract_and_plot_first_frame(video_path):\n",
    "    print(f\"Attempting to open video: {video_path}\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file '{video_path}'\")\n",
    "        return\n",
    "    ret, frame_array = cap.read() # The frame is automatically stored as a NumPy array\n",
    "    if ret:\n",
    "        rgb_frame = cv2.cvtColor(frame_array, cv2.COLOR_BGR2RGB)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(rgb_frame)\n",
    "        plt.title(f\"First Frame of Video: {os.path.basename(video_path)}\")\n",
    "        plt.axis('off') # Hide axis ticks\n",
    "        plt.show() # Display the plot\n",
    "        \n",
    "    else:\n",
    "        print(\"Error: Could not read the first frame from the video.\")\n",
    "    cap.release()\n",
    "\n",
    "# path = '/workspaces/proyecto/workspace/src_c40/Original'\n",
    "# video = glob.glob(os.path.join(path, '*.mp4'))[0]\n",
    "video_file = '/workspaces/proyecto/workspace/src_c40/Original/000.mp4'\n",
    "\n",
    "if os.path.exists(video_file):\n",
    "    extract_and_plot_first_frame(video_file)\n",
    "else:\n",
    "    print(f\"\\nNote: Video file '{video_file}' not found. Please ensure '{video_file}' exists in this directory or update the 'video_file' path.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
